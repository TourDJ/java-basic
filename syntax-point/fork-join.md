
## Fork join

### What's fork join?

In parallel computing, the fork–join model is a way of setting up and executing parallel programs, such that execution branches off in parallel at designated points in the program, to "join" (merge) at a subsequent point and resume sequential execution. Parallel sections may fork recursively until a certain task granularity is reached. Fork–join can be considered a parallel design pattern.

Below is the author of jdk fork-join's Doug Lea give explain:

### Implementation Overview

The `ForkJoinPool` class and its nested classes provide the main functionality and control for a set of worker threads: Submissions from non-FJ threads enter into submission queues. Workers take these tasks and typically split them into subtasks that may be stolen by other workers.  Preference rules give first priority to processing tasks from their own queues (LIFO or FIFO, depending on mode), then to randomized FIFO steals of tasks in other queues.  This framework began as vehicle for supporting tree-structured parallelism using work-stealing. Over time, its scalability advantages led to extensions and changes to better support more diverse usage contexts.  Because most internal methods and nested classes are interrelated, their main rationale and descriptions are presented here individual methods and nested classes contain only brief comments about details.

#### WorkQueues

Most operations occur within work-stealing queues (in nested class WorkQueue).  These are special forms of Deques that support only three of the four possible end-operations -- push, pop, and poll (aka steal), under the further constraints that push and pop are called only from the owning thread (or, as extended here, under a lock), while poll may be called from other threads.  (If you are unfamiliar with them, you probably want to read Herlihy and Shavit's book "The Art of Multiprocessor programming", chapter 16 describing these in more detail before proceeding.)  The main work-stealing queue design is roughly similar to those in the papers "Dynamic Circular Work-Stealing Deque" by Chase and Lev, SPAA 2005 (http://research.sun.com/scalable/pubs/index.html) and "Idempotent work stealing" by Michael, Saraswat, and Vechev, PPoPP 2009 (http://portal.acm.org/citation.cfm?id=1504186).

The main differences ultimately stem from GC requirements that we null out taken slots as soon as we can, to maintain as small a footprint as possible even in programs generating huge numbers of tasks. To accomplish this, we shift the CAS arbitrating pop vs poll (steal) from being on the indices ("base" and "top") to the slots themselves.

Adding tasks then takes the form of a classic array push(task):
```java
   q.array[q.top] = task; ++q.top;
```
(The actual code needs to null-check and size-check the array,properly fence the accesses, and possibly signal waiting
workers to start scanning -- see below.)  Both a successful pop and poll mainly entail a CAS of a slot from non-null to null.

The pop operation (always performed by owner) is:
```java
  if ((base != top) and
       (the task at top slot is not null) and
       (CAS slot to null))
          decrement top and return task;
```
And the poll operation (usually by a stealer) is
```java
   if ((base != top) and
       (the task at base slot is not null) and
       (base has not changed) and
       (CAS slot to null))
          increment base and return task;
```
Because we rely on CASes of references, we do not need tag bits on base or top.  They are simple ints as used in any circular array-based queue (see for example ArrayDeque).  Updates to the indices guarantee that top == base means the queue is empty, but otherwise may err on the side of possibly making the queue appear nonempty when a push, pop, or poll have not fully committed. (Method isEmpty() checks the case of a partially completed removal of the last element.)  Because of this, the poll operation, considered individually, is not wait-free. One thief cannot successfully continue until another in-progress one (or, if previously empty, a push) completes.  However, in the aggregate, we ensure at least probabilistic non-blockingness.  If an attempted steal fails, a thief always chooses a different random victim target to try next. So, in order for one thief to progress, it suffices for any in-progress poll or new push on any empty queue to complete. (This is why we normally use method pollAt and its variants that try once at the apparent base index, else consider alternative actions, rather than method poll, which retries.)

This approach also enables support of a user mode in which local task processing is in FIFO, not LIFO order, simply by using poll rather than pop.  This can be useful in message-passing frameworks in which tasks are never joined. However neither mode considers affinities, loads, cache localities, etc, so rarely provide the best possible performance on a given machine, but portably provide good throughput by averaging over these factors.  Further, even if we did try to use such information, we do not usually have a basis for exploiting it.  For example, some sets of tasks profit from cache affinities, but others are harmed by cache pollution effects. Additionally, even though it requires scanning, long-term throughput is often best using random selection rather than directed selection policies, so cheap randomization of sufficient quality is used whenever applicable.  Various Marsaglia XorShifts (some with different shift constants) are inlined at use points.

WorkQueues are also used in a similar way for tasks submitted to the pool. We cannot mix these tasks in the same queues used by workers. Instead, we randomly associate submission queues with submitting threads, using a form of hashing.  The ThreadLocalRandom probe value serves as a hash code for choosing existing queues, and may be randomly repositioned upon contention with other submitters.  In essence, submitters act like workers except that they are restricted to executing local tasks that they submitted (or in the case of CountedCompleters, others with the same root task).  Insertion of tasks in shared mode requires a lock (mainly to protect in the case of resizing) but we use only a simple spinlock (using field qlock), because submitters encountering a busy queue move on to try or create other queues -- they block only when creating and registering new queues. Additionally, "qlock" saturates to an unlockable value (-1) at shutdown. Unlocking still can be and is performed by cheaper ordered writes of "qlock" in successful cases, but uses CAS in unsuccessful cases.


#### Management

The main throughput advantages of work-stealing stem from decentralized control -- workers mostly take tasks from themselves or each other, at rates that can exceed a billion per second.  The pool itself creates, activates (enables scanning for and running tasks), deactivates, blocks, and terminates threads, all with minimal central information. There are only a few properties that we can globally track or maintain, so we pack them into a small number of variables, often maintaining atomicity without blocking or locking. Nearly all essentially atomic control state is held in two volatile variables that are by far most often read (not written) as status and consistency checks. (Also, field "config" holds unchanging configuration state.)

Field "ctl" contains 64 bits holding information needed to atomically decide to add, inactivate, enqueue (on an event queue), dequeue, and/or re-activate workers.  To enable this packing, we restrict maximum parallelism to (1<<15)-1 (which is far in excess of normal operating range) to allow ids, counts, and their negations (used for thresholding) to fit into 16bit subfields.

Field "runState" holds lockable state bits (STARTED, STOP, etc) also protecting updates to the workQueues array.  When used as a lock, it is normally held only for a few instructions (the only exceptions are one-time array initialization and uncommon resizing), so is nearly always available after at most a brief spin. But to be extra-cautious, after spinning, method awaitRunStateLock (called only if an initial CAS fails), uses a wait/notify mechanics on a builtin monitor to block when (rarely) needed. This would be a terrible idea for a highly contended lock, but most pools run without the lock ever contending after the spin limit, so this works fine as a more conservative alternative. Because we don't otherwise have an internal Object to use as a monitor, the "stealCounter" (an AtomicLong) is used when available (it too must be lazily initialized; see externalSubmit).

Usages of "runState" vs "ctl" interact in only one case: deciding to add a worker thread (see tryAddWorker), in which case the ctl CAS is performed while the lock is held.

Recording WorkQueues.  WorkQueues are recorded in the "workQueues" array. The array is created upon first use (see externalSubmit) and expanded if necessary.  Updates to the array while recording new workers and unrecording terminated ones are protected from each other by the runState lock, but the array is otherwise concurrently readable, and accessed directly. We also ensure that reads of the array reference itself never become too stale. To simplify index-based operations, the array size is always a power of two, and all readers must tolerate null slots. Worker queues are at odd indices. Shared (submission) queues are at even indices, up to a maximum of 64 slots, to limit growth even if array needs to expand to add more workers. Grouping them together in this way simplifies and speeds up task scanning.

All worker thread creation is on-demand, triggered by task submissions, replacement of terminated workers, and/or compensation for blocked workers. However, all other support code is set up to work with other policies.  To ensure that we do not hold on to worker references that would prevent GC, All accesses to workQueues are via indices into the workQueues array (which is one source of some of the messy code constructions here). In essence, the workQueues array serves as a weak reference mechanism. Thus for example the stack top subfield of ctl stores indices, not references.

Queuing Idle Workers. Unlike HPC work-stealing frameworks, we cannot let workers spin indefinitely scanning for tasks when none can be found immediately, and we cannot start/resume workers unless there appear to be tasks available.  On the other hand, we must quickly prod them into action when new tasks are submitted or generated. In many usages, ramp-up time to activate workers is the main limiting factor in overall performance, which is compounded at program start-up by JIT compilation and allocation. So we streamline this as much as possible.

The "ctl" field atomically maintains active and total worker counts as well as a queue to place waiting threads so they can be located for signalling. Active counts also play the role of quiescence indicators, so are decremented when workers believe that there are no more tasks to execute. The "queue" is actually a form of Treiber stack.  A stack is ideal for activating threads in most-recently used order. This improves performance and locality, outweighing the disadvantages of being prone to contention and inability to release a worker unless it is topmost on stack.  We park/unpark workers after pushing on the idle worker stack (represented by the lower 32bit subfield of ctl) when they cannot find work.  The top stack state holds the value of the "scanState" field of the worker: its index and status, plus a version counter that, in addition to the count subfields (also serving as version stamps) provide protection against Treiber stack ABA effects.

Field scanState is used by both workers and the pool to manage and track whether a worker is INACTIVE (possibly blocked waiting for a signal), or SCANNING for tasks (when neither hold it is busy running tasks).  When a worker is inactivated, its scanState field is set, and is prevented from executing tasks, even though it must scan once for them to avoid queuing races. Note that scanState updates lag queue CAS releases so usage requires care. When queued, the lower 16 bits of scanState must hold its pool index. So we place the index there upon initialization (see registerWorker) and otherwise keep it there or restore it when necessary.

Memory ordering.  See "Correct and Efficient Work-Stealing for Weak Memory Models" by Le, Pop, Cohen, and Nardelli, PPoPP 2013 (http://www.di.ens.fr/~zappa/readings/ppopp13.pdf) for an analysis of memory ordering requirements in work-stealing algorithms similar to the one used here.  We usually need stronger than minimal ordering because we must sometimes signal workers, requiring Dekker-like full-fences to avoid lost signals.  Arranging for enough ordering without expensive over-fencing requires tradeoffs among the supported means of expressing access constraints. The most central operations, taking from queues and updating ctl state, require full-fence CAS.  Array slots are read using the emulation of volatiles provided by Unsafe.  Access from other threads to WorkQueue base, top, and array requires a volatile load of the first of any of these read.  We use the convention of declaring the "base" index volatile, and always read it before other fields. The owner thread must ensure ordered updates, so writes use ordered intrinsics unless they can piggyback on those for other writes.  Similar conventions and rationales hold for other WorkQueue fields (such as "currentSteal") that are only written by owners but observed by others.

Creating workers. To create a worker, we pre-increment total count (serving as a reservation), and attempt to construct a ForkJoinWorkerThread via its factory. Upon construction, the new thread invokes registerWorker, where it constructs a WorkQueue and is assigned an index in the workQueues array (expanding the array if necessary). The thread is then started. Upon any exception across these steps, or null return from factory, deregisterWorker adjusts counts and records accordingly.  If a null return, the pool continues running with fewer than the target number workers. If exceptional, the exception is propagated, generally to some external caller. Worker index assignment avoids the bias in scanning that would occur if entries were sequentially packed starting at the front of the workQueues array. We treat the array as a simple power-of-two hash table, expanding as needed. The seedIndex increment ensures no collisions until a resize is needed or a worker is deregistered and replaced, and thereafter keeps probability of collision low. We cannot use ThreadLocalRandom.getProbe() for similar purposes here because the thread has not started yet, but do so for creating submission queues for existing external threads.

Deactivation and waiting. Queuing encounters several intrinsic races; most notably that a task-producing thread can miss seeing (and signalling) another thread that gave up looking for work but has not yet entered the wait queue.  When a worker cannot find a task to steal, it deactivates and enqueues. Very often, the lack of tasks is transient due to GC or OS scheduling. To reduce false-alarm deactivation, scanners compute checksums of queue states during sweeps.  (The stability checks used here and elsewhere are probabilistic variants of snapshot techniques -- see Herlihy & Shavit.) Workers give up and try to deactivate only after the sum is stable across scans. Further, to avoid missed signals, they repeat this scanning process after successful enqueuing until again stable.  In this state, the worker cannot take/run a task it sees until it is released from the queue, so the worker itself eventually tries to release itself or any successor (see tryRelease).  Otherwise, upon an empty scan, a deactivated worker uses an adaptive local spin construction (see awaitWork) before blocking (via park). Note the unusual conventions about Thread.interrupts surrounding parking and other blocking: Because interrupts are used solely to alert threads to check termination, which is checked anyway upon blocking, we clear status (using Thread.interrupted) before any call to park, so that park does not immediately return due to status being set via some other unrelated call to interrupt in user code.

Signalling and activation.  Workers are created or activated only when there appears to be at least one task they might be able to find and execute.  Upon push (either by a worker or an external submission) to a previously (possibly) empty queue, workers are signalled if idle, or created if fewer exist than the given parallelism level.  These primary signals are buttressed by others whenever other threads remove a task from a queue and notice that there are other tasks there as well. On most platforms, signalling (unpark) overhead time is noticeably long, and the time between signalling a thread and it actually making progress can be very noticeably long, so it is worth offloading these delays from critical paths as much as possible. Also, because inactive workers are often rescanning or spinning rather than blocking, we set and clear the "parker" field of WorkQueues to reduce unnecessary calls to unpark. (This requires a secondary recheck to avoid missed signals.)

Trimming workers. To release resources after periods of lack of use, a worker starting to wait when the pool is quiescent will time out and terminate (see awaitWork) if the pool has remained quiescent for period IDLE_TIMEOUT, increasing the period as the number of threads decreases, eventually removing all workers. Also, when more than two spare threads exist, excess threads are immediately terminated at the next quiescent point. (Padding by two avoids hysteresis.)

Shutdown and Termination. A call to shutdownNow invokes tryTerminate to atomically set a runState bit. The calling thread, as well as every other worker thereafter terminating, helps terminate others by setting their (qlock) status, cancelling their unprocessed tasks, and waking them up, doing so repeatedly until stable (but with a loop bounded by the number of workers).  Calls to non-abrupt shutdown() preface this by checking whether termination should commence. This relies primarily on the active count bits of "ctl" maintaining consensus -- tryTerminate is called from awaitWork whenever quiescent. However, external submitters do not take part in this consensus.  So, tryTerminate sweeps through queues (until stable) to ensure lack of in-flight submissions and workers about to process them before triggering the "STOP" phase of termination. (Note: there is an intrinsic conflict if helpQuiescePool is called when shutdown is enabled. Both wait for quiescence, but tryTerminate is biased to not trigger until helpQuiescePool completes.)

#### Joining Tasks

Any of several actions may be taken when one worker is waiting to join a task stolen (or always held) by another.  Because we are multiplexing many tasks on to a pool of workers, we can't just let them block (as in Thread.join).  We also cannot just reassign the joiner's run-time stack with another and replace it later, which would be a form of "continuation", that even if possible is not necessarily a good idea since we may need both an unblocked task and its continuation to progress.  Instead we combine two tactics:

>  Helping: Arranging for the joiner to execute some task that it would be running if the steal had not occurred.

>  Compensating: Unless there are already enough live threads, method tryCompensate() may create or re-activate a spare thread to compensate for blocked joiners until they unblock.

A third form (implemented in tryRemoveAndExec) amounts to helping a hypothetical compensator: If we can readily tell that a possible action of a compensator is to steal and execute the task being joined, the joining thread can do so directly, without the need for a compensation thread (although at the expense of larger run-time stacks, but the tradeoff is typically worthwhile).

The ManagedBlocker extension API can't use helping so relies only on compensation in method awaitBlocker.

The algorithm in helpStealer entails a form of "linear helping".  Each worker records (in field currentSteal) the most recent task it stole from some other worker (or a submission). It also records (in field currentJoin) the task it is currently actively joining. Method helpStealer uses these markers to try to find a worker to help (i.e., steal back a task from and execute it) that could hasten completion of the actively joined task.  Thus, the joiner executes a task that would be on its own local deque had the to-be-joined task not been stolen. This is a conservative variant of the approach described in Wagner & Calder "Leapfrogging: a portable technique for implementing efficient futures" SIGPLAN Notices, 1993 (http://portal.acm.org/citation.cfm?id=155354). It differs in that: (1) We only maintain dependency links across workers upon steals, rather than use per-task bookkeeping.  This sometimes requires a linear scan of workQueues array to locate stealers, but often doesn't because stealers leave hints (that may become stale/wrong) of where to locate them.  It is only a hint because a worker might have had multiple steals and the hint records only one of them (usually the most current).  Hinting isolates cost to when it is needed, rather than adding to per-task overhead.  (2) It is "shallow", ignoring nesting and potentially cyclic mutual steals.  (3) It is intentionally racy: field currentJoin is updated only while actively joining, which means that we miss links in the chain during long-lived tasks, GC stalls etc (which is OK since blocking in such cases is usually a good idea).  (4) We bound the number of attempts to find work using checksums and fall back to suspending the worker and if necessary replacing it with another.

Helping actions for CountedCompleters do not require tracking currentJoins: Method helpComplete takes and executes any task with the same root as the task being waited on (preferring local pops to non-local polls). However, this still entails some traversal of completer chains, so is less efficient than using CountedCompleters without explicit joins.

Compensation does not aim to keep exactly the target parallelism number of unblocked threads running at any given time. Some previous versions of this class employed immediate compensations for any blocked join. However, in practice, the vast majority of blockages are transient byproducts of GC and other JVM or OS activities that are made worse by replacement. Currently, compensation is attempted only after validating that all purportedly active threads are processing tasks by checking field WorkQueue.scanState, which eliminates most false positives.  Also, compensation is bypassed (tolerating fewer threads) in the most common case in which it is rarely beneficial: when a worker with an empty queue (thus no continuation tasks) blocks on a join and there still remain enough threads to ensure liveness.

The compensation mechanism may be bounded.  Bounds for the commonPool (see commonMaxSpares) better enable JVMs to cope with programming errors and abuse before running out of resources to do so. In other cases, users may supply factories that limit thread construction. The effects of bounding in this pool (like all others) is imprecise.  Total worker counts are decremented when threads deregister, not when they exit and resources are reclaimed by the JVM and OS. So the number of simultaneously live threads may transiently exceed bounds.

#### Common Pool

The static common pool always exists after static initialization.  Since it (or any other created pool) need never be used, we minimize initial construction overhead and footprint to the setup of about a dozen fields, with no nested allocation. Most bootstrapping occurs within method externalSubmit during the first submission to the pool.

When external threads submit to the common pool, they can perform subtask processing (see externalHelpComplete and related methods) upon joins.  This caller-helps policy makes it sensible to set common pool parallelism level to one (or more) less than the total number of available cores, or even zero for pure caller-runs.  We do not need to record whether external submissions are to the common pool -- if not, external help methods return quickly. These submitters would otherwise be blocked waiting for completion, so the extra effort (with liberally sprinkled task status checks) in inapplicable cases amounts to an odd form of limited spin-wait before blocking in ForkJoinTask.join.

As a more appropriate default in managed environments, unless overridden by system properties, we use workers of subclass InnocuousForkJoinWorkerThread when there is a SecurityManager present. These workers have no permissions set, do not belong to any user-defined ThreadGroup, and erase all ThreadLocals after executing any top-level task (see WorkQueue.runTask). The associated mechanics (mainly in ForkJoinWorkerThread) may be JVM-dependent and must access particular Thread class fields to achieve this effect.


#### Style notes

Memory ordering relies mainly on Unsafe intrinsics that carry the further responsibility of explicitly performing null- and bounds- checks otherwise carried out implicitly by JVMs.  This can be awkward and ugly, but also reflects the need to control outcomes across the unusual cases that arise in very racy code with very few invariants. So these explicit checks would exist in some form anyway.  All fields are read into locals before use, and null-checked if they are references.  This is usually done in a "C"-like style of listing declarations at the heads of methods or blocks, and using inline assignments on first encounter.  Array bounds-checks are usually performed by masking with array.length-1, which relies on the invariant that these arrays are created with positive lengths, which is itself paranoically checked. Nearly all explicit checks lead to bypass/return, not exception throws, because they may legitimately arise due to cancellation/revocation during shutdown.

There is a lot of representation-level coupling among classes ForkJoinPool, ForkJoinWorkerThread, and ForkJoinTask.  The fields of WorkQueue maintain data structures managed by ForkJoinPool, so are directly accessed.  There is little point trying to reduce this, since any associated future changes in representations will need to be accompanied by algorithmic changes anyway. Several methods intrinsically sprawl because they must accumulate sets of consistent reads of fields held in local variables.  There are also other coding oddities (including several unnecessary-looking hoisted null checks) that help some methods perform reasonably even when interpreted(not compiled).

The order of declarations in this file is (with a few exceptions):       
* (1) Static utility functions
* (2) Nested (static) classes
* (3) Static fields
* (4) Fields, along with constants used when unpacking some of them
* (5) Internal control methods
* (6) Callbacks and other support for ForkJoinTask methods
* (7) Exported methods
* (8) Static block initializing statics in minimally dependent order
